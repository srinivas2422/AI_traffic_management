{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7700f53",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [2]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fd29562-9fb9-40b9-9eae-e9f1fe36e0fe",
   "metadata": {
    "papermill": {
     "duration": 0.014793,
     "end_time": "2024-12-09T06:58:21.802723",
     "exception": false,
     "start_time": "2024-12-09T06:58:21.787930",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the absolute path of the src folder to sys.path\n",
    "sys.path.append(os.path.abspath(r'C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\src'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92339ca",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "492bacd2-ce8c-4f25-84d6-11743849dce7",
   "metadata": {
    "papermill": {
     "duration": 9.771413,
     "end_time": "2024-12-09T06:58:31.576168",
     "exception": true,
     "start_time": "2024-12-09T06:58:21.804755",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "from data_processing import extract_frames\n",
    "from vehicle_detection import detect_vehicles\n",
    "\n",
    "# Initialize variables for vehicle tracking\n",
    "vehicle_positions = {}  # Dictionary to track vehicle positions across frames\n",
    "vehicle_speeds = deque(maxlen=10)  # Store the speed of tracked vehicles\n",
    "frame_rate = 2  # Assuming 2 frames per second for speed calculation\n",
    "previous_vehicles = []  # Placeholder for previous vehicle positions\n",
    "previous_frame = None  # Placeholder for previous frame\n",
    "\n",
    "def calculate_average_speed(frame_path, previous_vehicles, previous_frame):\n",
    "    \"\"\"Estimate the average speed of vehicles in the frame using vehicle tracking.\"\"\"\n",
    "    current_vehicle_count, current_boxes = detect_vehicles(frame_path)\n",
    "\n",
    "    total_speed = 0\n",
    "    speed_count = 0\n",
    "\n",
    "    if previous_frame is not None and len(previous_vehicles) > 0:\n",
    "        for i, box in enumerate(current_boxes):\n",
    "            # Extract bounding box coordinates and convert them to float\n",
    "            x1, y1, x2, y2 = box.xywh[0].tolist()  # Convert Tensor to list and unpack\n",
    "            current_position = (x1 + x2) / 2  # Take the middle point of the bounding box\n",
    "\n",
    "            if i < len(previous_vehicles):\n",
    "                previous_position = previous_vehicles[i]\n",
    "                speed = abs(current_position - previous_position) * frame_rate  # Speed = distance/time\n",
    "                total_speed += speed\n",
    "                speed_count += 1\n",
    "\n",
    "        if speed_count > 0:\n",
    "            return round(total_speed / speed_count, 2)\n",
    "    return 0\n",
    "\n",
    "def calculate_density(frame_path,vehicle_count):\n",
    "    \"\"\"Calculate traffic density in the frame.\"\"\"\n",
    "    img = cv2.imread(frame_path)\n",
    "    height, width, _ = img.shape\n",
    "    total_area = height * width\n",
    "\n",
    "    if total_area > 0:\n",
    "        density = vehicle_count / (total_area / 100000)  # Normalize to vehicles per 100,000 pixels\n",
    "        return round(density, 6)\n",
    "    return 0\n",
    "\n",
    "def calculate_queue_length(frame_path):\n",
    "    \"\"\"Estimate the queue length of stationary vehicles in the frame.\"\"\"\n",
    "    _, current_boxes = detect_vehicles(frame_path)\n",
    "\n",
    "    vehicle_positions_in_line = []\n",
    "\n",
    "    for box in current_boxes:\n",
    "        # Extract bounding box coordinates and convert to float\n",
    "        x1, _, x2, _ = box.xywh[0].tolist()  # Use .tolist() to convert Tensor to a list of values\n",
    "        vehicle_positions_in_line.append((x1 + x2) / 2)  # Take the midpoint of the vehicle\n",
    "\n",
    "    vehicle_positions_in_line.sort()\n",
    "\n",
    "    if len(vehicle_positions_in_line) > 1:\n",
    "        queue_length = (vehicle_positions_in_line[-1] - vehicle_positions_in_line[0]) * 0.02  # Scaling factor\n",
    "    else:\n",
    "        queue_length = 0\n",
    "\n",
    "    return round(queue_length, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "807cc204-7498-4447-809d-bafeda48dd15",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "from ultralytics import YOLO\n",
    "from collections import deque\n",
    "from data_processing import extract_frames\n",
    "from vehicle_detection import detect_vehicles\n",
    "from emergency_detection import detect_emergency_vehicles \n",
    "\n",
    "def process_video(video_path, output_frames_folder, output_metrics_folder, model_path=\"../models/yolo11n.pt\", frame_rate=1):\n",
    "    \"\"\"\n",
    "    Process a video to extract frames, calculate metrics (including emergency vehicles), and save results to CSV.\n",
    "    \"\"\"\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    video_frames_folder = os.path.join(output_frames_folder, video_name)\n",
    "    video_metrics_folder = os.path.join(output_metrics_folder, video_name)\n",
    "\n",
    "    # Create necessary folders if not already present\n",
    "    os.makedirs(video_frames_folder, exist_ok=True)\n",
    "    os.makedirs(video_metrics_folder, exist_ok=True)\n",
    "    \n",
    "    # Step 1: Extract frames from the video\n",
    "    extract_frames(video_path, video_frames_folder, frame_rate)\n",
    "\n",
    "    # Step 2: Calculate metrics for each frame and save to CSV\n",
    "    metrics = []\n",
    "    previous_vehicles = []\n",
    "    previous_frame = None\n",
    "\n",
    "    for frame_file in sorted(os.listdir(video_frames_folder)):  # Ensure frame order\n",
    "        frame_path = os.path.join(video_frames_folder, frame_file)\n",
    "\n",
    "        # Calculate metrics\n",
    "        vehicle_count, current_boxes = detect_vehicles(frame_path, model_path)\n",
    "        avg_speed = calculate_average_speed(frame_path, previous_vehicles, previous_frame)\n",
    "        queue_length = calculate_queue_length(frame_path)\n",
    "        density = calculate_density(frame_path,vehicle_count)\n",
    "\n",
    "        emergency = detect_emergency_vehicles(frame_path)\n",
    "\n",
    "        # Append all metrics\n",
    "        metrics.append({\n",
    "            \"Frame\": frame_file,\n",
    "            \"Number_of_Vehicles\": vehicle_count,\n",
    "            \"Average_Speed_km/h\": avg_speed,\n",
    "            \"Traffic_Density\": density,\n",
    "            \"Queue_Length_meters\": queue_length,\n",
    "            \"Emergency_Vehicles\": emergency\n",
    "        })\n",
    "\n",
    "        # Update previous vehicles and previous frame for the next iteration\n",
    "        previous_vehicles = [(box.xywh[0][0].item() + box.xywh[0][2].item()) / 2 for box in current_boxes]  # Convert tensor to float\n",
    "        previous_frame = cv2.imread(frame_path)  # Store the current frame as the previous frame\n",
    "\n",
    "    # Step 3: Save metrics to CSV\n",
    "    metrics_file_path = os.path.join(video_metrics_folder, f\"{video_name}_metrics.csv\")\n",
    "    with open(metrics_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=[\"Frame\", \"Number_of_Vehicles\", \"Average_Speed_km/h\", \"Traffic_Density\", \"Queue_Length_meters\", \"Emergency_Vehicles\"])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(metrics)\n",
    "    \n",
    "    print(f\"Metrics for {video_name} saved to {metrics_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60410e2d-60a4-4315-ba6c-b47974af9cec",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames extracted to ../outputs/frames/video1\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0000.jpg: 384x640 3 persons, 16 cars, 2 motorcycles, 182.2ms\n",
      "Speed: 9.5ms preprocess, 182.2ms inference, 11.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0000.jpg: 384x640 3 persons, 16 cars, 2 motorcycles, 99.2ms\n",
      "Speed: 3.0ms preprocess, 99.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0000.jpg: 384x640 3 persons, 16 cars, 2 motorcycles, 98.9ms\n",
      "Speed: 2.0ms preprocess, 98.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0000.jpg: 384x640 3 cars, 105.9ms\n",
      "Speed: 2.0ms preprocess, 105.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0025.jpg: 384x640 5 persons, 17 cars, 4 motorcycles, 1 bus, 2 trucks, 90.5ms\n",
      "Speed: 2.0ms preprocess, 90.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0025.jpg: 384x640 5 persons, 17 cars, 4 motorcycles, 1 bus, 2 trucks, 86.1ms\n",
      "Speed: 3.0ms preprocess, 86.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0025.jpg: 384x640 5 persons, 17 cars, 4 motorcycles, 1 bus, 2 trucks, 78.0ms\n",
      "Speed: 2.0ms preprocess, 78.0ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0025.jpg: 384x640 2 cars, 83.6ms\n",
      "Speed: 2.0ms preprocess, 83.6ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0050.jpg: 384x640 5 persons, 21 cars, 4 motorcycles, 5 trucks, 81.8ms\n",
      "Speed: 2.0ms preprocess, 81.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0050.jpg: 384x640 5 persons, 21 cars, 4 motorcycles, 5 trucks, 75.6ms\n",
      "Speed: 3.9ms preprocess, 75.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0050.jpg: 384x640 5 persons, 21 cars, 4 motorcycles, 5 trucks, 72.5ms\n",
      "Speed: 2.0ms preprocess, 72.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0050.jpg: 384x640 3 cars, 80.1ms\n",
      "Speed: 2.0ms preprocess, 80.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0075.jpg: 384x640 8 persons, 17 cars, 4 motorcycles, 2 trucks, 76.5ms\n",
      "Speed: 4.0ms preprocess, 76.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0075.jpg: 384x640 8 persons, 17 cars, 4 motorcycles, 2 trucks, 71.5ms\n",
      "Speed: 2.0ms preprocess, 71.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0075.jpg: 384x640 8 persons, 17 cars, 4 motorcycles, 2 trucks, 83.1ms\n",
      "Speed: 1.0ms preprocess, 83.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0075.jpg: 384x640 3 cars, 70.6ms\n",
      "Speed: 3.0ms preprocess, 70.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0100.jpg: 384x640 7 persons, 15 cars, 4 motorcycles, 6 trucks, 79.3ms\n",
      "Speed: 4.0ms preprocess, 79.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0100.jpg: 384x640 7 persons, 15 cars, 4 motorcycles, 6 trucks, 74.6ms\n",
      "Speed: 2.0ms preprocess, 74.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0100.jpg: 384x640 7 persons, 15 cars, 4 motorcycles, 6 trucks, 73.5ms\n",
      "Speed: 1.0ms preprocess, 73.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "image 1/1 C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\..\\outputs\\frames\\video1\\frame_0100.jpg: 384x640 3 cars, 68.0ms\n",
      "Speed: 3.0ms preprocess, 68.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m output_metrics_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../outputs/metrics/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_path \u001b[38;5;129;01min\u001b[39;00m video_paths:\n\u001b[1;32m---> 12\u001b[0m     \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_frames_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_metrics_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 34\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, output_frames_folder, output_metrics_folder, model_path, frame_rate)\u001b[0m\n\u001b[0;32m     31\u001b[0m frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(video_frames_folder, frame_file)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m vehicle_count, current_boxes \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_vehicles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m avg_speed \u001b[38;5;241m=\u001b[39m calculate_average_speed(frame_path, previous_vehicles, previous_frame)\n\u001b[0;32m     36\u001b[0m queue_length \u001b[38;5;241m=\u001b[39m calculate_queue_length(frame_path)\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\AI_traffic_management\\ML\\src\\vehicle_detection.py:8\u001b[0m, in \u001b[0;36mdetect_vehicles\u001b[1;34m(frame_path, model_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m yolo_model \u001b[38;5;241m=\u001b[39m YOLO(model_path)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Perform detection\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43myolo_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m vehicle_boxes \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mboxes  \u001b[38;5;66;03m# All bounding boxes detected\u001b[39;00m\n\u001b[0;32m     11\u001b[0m vehicle_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(vehicle_boxes)  \u001b[38;5;66;03m# Total number of detected vehicles\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:176\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    149\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    150\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    152\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py:547\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor \u001b[38;5;241m=\u001b[39m (predictor \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_smart_load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredictor\u001b[39m\u001b[38;5;124m\"\u001b[39m))(overrides\u001b[38;5;241m=\u001b[39margs, _callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks)\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_cli\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# only update args if predictor is already setup\u001b[39;00m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m get_cfg(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39margs, args)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\predictor.py:304\u001b[0m, in \u001b[0;36mBasePredictor.setup_model\u001b[1;34m(self, model, verbose)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    303\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Initialize YOLO model with given parameters and set it to evaluation mode.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mAutoBackend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselect_device\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfuse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice  \u001b[38;5;66;03m# update device\u001b[39;00m\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mhalf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfp16  \u001b[38;5;66;03m# update half\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\autobackend.py:148\u001b[0m, in \u001b[0;36mAutoBackend.__init__\u001b[1;34m(self, weights, device, dnn, data, fp16, batch, fuse, verbose)\u001b[0m\n\u001b[0;32m    146\u001b[0m model \u001b[38;5;241m=\u001b[39m weights\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fuse:\n\u001b[1;32m--> 148\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfuse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkpt_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    150\u001b[0m     kpt_shape \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mkpt_shape  \u001b[38;5;66;03m# pose-only\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\nn\\tasks.py:207\u001b[0m, in \u001b[0;36mBaseModel.fuse\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, Conv2):\n\u001b[0;32m    206\u001b[0m     m\u001b[38;5;241m.\u001b[39mfuse_convs()\n\u001b[1;32m--> 207\u001b[0m m\u001b[38;5;241m.\u001b[39mconv \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_and_bn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# update conv\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mdelattr\u001b[39m(m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# remove batchnorm\u001b[39;00m\n\u001b[0;32m    209\u001b[0m m\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mforward_fuse  \u001b[38;5;66;03m# update forward\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\utils\\torch_utils.py:245\u001b[0m, in \u001b[0;36mfuse_conv_and_bn\u001b[1;34m(conv, bn)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfuse_conv_and_bn\u001b[39m(conv, bn):\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fuse Conv2d() and BatchNorm2d() layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/.\"\"\"\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     fusedconv \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 245\u001b[0m         \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m         \u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;241m.\u001b[39mto(conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Prepare filters\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     w_conv \u001b[38;5;241m=\u001b[39m conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mview(conv\u001b[38;5;241m.\u001b[39mout_channels, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:445\u001b[0m, in \u001b[0;36mConv2d.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    443\u001b[0m padding_ \u001b[38;5;241m=\u001b[39m padding \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m _pair(padding)\n\u001b[0;32m    444\u001b[0m dilation_ \u001b[38;5;241m=\u001b[39m _pair(dilation)\n\u001b[1;32m--> 445\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:132\u001b[0m, in \u001b[0;36m_ConvNd.__init__\u001b[1;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(\n\u001b[0;32m    130\u001b[0m         (in_channels, out_channels \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m groups, \u001b[38;5;241m*\u001b[39mkernel_size), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_channels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_channels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_channels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "video_paths = [\n",
    "    \"../data/videos/video1.mp4\", \n",
    "    \"../data/videos/video2.mp4\", \n",
    "    \"../data/videos/video3.mp4\", \n",
    "    \"../data/videos/video4.mp4\"\n",
    "]\n",
    "\n",
    "output_frames_folder = \"../outputs/frames/\"\n",
    "output_metrics_folder = \"../outputs/metrics/\"\n",
    "\n",
    "for video_path in video_paths:\n",
    "    process_video(video_path, output_frames_folder, output_metrics_folder, frame_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f30fc-3cb1-4dc0-a2d5-b4c7cd88f255",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.823473,
   "end_time": "2024-12-09T06:58:32.462997",
   "environment_variables": {},
   "exception": true,
   "input_path": "C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\data_preprocessing.ipynb",
   "output_path": "C:\\Users\\asr12\\OneDrive\\Desktop\\AI_traffic_management\\ML\\notebooks\\data_preprocessing.ipynb",
   "parameters": {},
   "start_time": "2024-12-09T06:58:18.639524",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
